# Jan 30 2026 - What to do the following week

## 1.  K8s vs k3s;  
### try out k3s, and see if anything similar or different
Check out the k3s document!
I would say k3s is a bit low-cost.

## 2. Containerization app, build an image, consume the cpu 
### try to build a new image, and write a script or an api that consumes cpu(set to 50), try to run HPA see if anything should be happening happens(the scaling, the cpu usage raise up as expected)

#### 1. Build own image

```bash
cd image
docker build -t cpu-nginx:0.1 .
cd ..
```
### 2.add it into k8s for testing

Create cluster

```bash
kind create cluster --config cluster_config/cluster.k8s.yaml --wait 5m
```

deploy metricsServer

```bash
kubectl apply -f ./metricsServer_config/metrics_service.yaml
kubectl -n kube-system rollout status deploy/metrics-server
```

check to see if successfully installed:

```bash
kubectl top nodes
kubectl top pods
```

Before deploying pods, see if you're in the right cluster. If not, switch to the correct cluster using command:

```bash
kubectl config get-contexts
kubectl config use-context kind-cluster
```

```bash
docker load -i ./image/cpu-nginx_0.1.tar
kind load docker-image cpu-nginx:0.1 --name cluster
kubectl apply -f ./deploy_config/deploy.k8s.complete.yaml
```

### 3. testing

```bash
chmod +x scripts/collect_hpa_50m.csv.sh
./scripts/collect_hpa_50m.csv.sh
```

Start 50% burn for 5 minutes or try to use cpu usage in one request:

```bash
curl "http://localhost:8082/burn?load=50&seconds=120"

curl "http://localhost:8081/work?ms=50"
```


less threads, less cup usage; closer to calculation

```bash
kubectl run -it --rm fortio --image=fortio/fortio --restart=Never -- \
  load -t 120s -c 20 -qps 500 -uniform -nocatchup -keepalive=false -loglevel Error \
  "http://cpu-nginx:8081/work?ms=1"
  
kubectl run -it --rm fortio --image=fortio/fortio --restart=Never -- \
  load -t 300s -c 6 -qps 30 -uniform -nocatchup -keepalive=false -loglevel Error \
  "http://cpu-nginx:8081/work?ms=10"
```


check status:

```bash
kubectl get hpa cpu-nginx-hpa --watch
```

Stop anytime if using burn:

```bash
curl "http://localhost:8081/stop"
```

## 3, metrics: cpu, replicas, timestamp, invocations, delay
### metrics that should be collected
#### 1. Sample HPA + replicas + CPU/mem into a CSV (every N seconds)
install jq (for parsing JSON):

```bash
brew install jq
```

run command:

```bash
chmod +x scripts/collect_hpa.csv.sh
CTX=kind-cluster INTERVAL=1 ./scripts/collect_hpa.csv.sh
```

#### 2. Save autoscaling events (scale up/down reasons) to a file

```bash
kubectl --context kind-cluster -n default get events \
  --field-selector involvedObject.kind=HorizontalPodAutoscaler,involvedObject.name=example1-hpa \
  -w -o json > ./collected_data/hpa_events.jsonl
```

## 4. Poisson distribution. 
### use the given dataset to simulate the real situation, and see if I can make the request fairly distributed in that one minute. 

## 5. Cool down time. 
### the scaling cool down time, manual setting to see if the scaling changes
Check out new setting documents, there's a setting.

![faster change.png](img/img.png)

## 6. 2 scripts, watch collect metrics, terminal. Script, request. 
### run two scripts in to terminals, and see if there's anything nicer that could make requests 

```bash
chmod +x scripts/replay_datasets_fortio.sh

#Dataset A only
./scripts/replay_datasets_fortio.sh ./dataset/combined_all_days.csv --datasets A --speed 1

#Dataset C only
./scripts/replay_datasets_fortio.sh ./dataset/combined_all_days.csv --datasets C --speed 1

#Both A and C together (A uses ms=1, C uses ms=2)
./scripts/replay_datasets_fortio.sh ./dataset/combined_all_days.csv --datasets A,C --speed 1 --conc-a 5 --conc-c 5
```